"""
Streamlit Frontend for Arabic/Darija Fake News Detection
Provides RTL Arabic interface for text analysis
"""

import streamlit as st
import requests
import json
import time
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import pandas as pd
import numpy as np
from datetime import datetime
import arabic_reshaper
import bidi.algorithm

# Configure page
st.set_page_config(
    page_title="ğŸ” Arabic/Darija Fake News Detection",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for RTL support
st.markdown("""
<style>
@import url('https://fonts.googleapis.com/css2?family=Tajawal:wght@400;500;700&display=swap');

/* RTL and Arabic font support */
.rtl-text {
    direction: rtl;
    text-align: right;
    font-family: 'Tajawal', 'Arial', sans-serif;
    line-height: 1.8;
}

.arabic-font {
    font-family: 'Tajawal', 'Arial', sans-serif;
}

/* Main container */
.main .block-container {
    direction: rtl;
    text-align: right;
    font-family: 'Tajawal', 'Arial', sans-serif;
}

/* Sidebar */
.sidebar .sidebar-content {
    direction: rtl;
    text-align: right;
    font-family: 'Tajawal', 'Arial', sans-serif;
}

/* Headers */
h1, h2, h3, h4, h5, h6 {
    font-family: 'Tajawal', 'Arial', sans-serif;
    direction: rtl;
    text-align: right;
}

/* Text areas and inputs */
.stTextArea, .stTextInput {
    direction: rtl;
    text-align: right;
    font-family: 'Tajawal', 'Arial', sans-serif;
    font-size: 16px;
}

/* Buttons */
.stButton > button {
    font-family: 'Tajawal', 'Arial', sans-serif;
    font-size: 16px;
    padding: 10px 20px;
}

/* Metrics */
.metric-card {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    padding: 20px;
    border-radius: 10px;
    color: white;
    text-align: center;
    margin: 10px 0;
    box-shadow: 0 4px 6px rgba(0,0,0,0.1);
}

.risk-low { background: linear-gradient(135deg, #2ecc71 0%, #27ae60 100%); }
.risk-medium { background: linear-gradient(135deg, #f39c12 0%, #e67e22 100%); }
.risk-high { background: linear-gradient(135deg, #e74c3c 0%, #c0392b 100%); }
.risk-very-high { background: linear-gradient(135deg, #8e44ad 0%, #2c3e50 100%); }

/* Feature importance chart */
.feature-bar {
    margin: 5px 0;
}

.explanation-box {
    background-color: #f8f9fa;
    padding: 20px;
    border-radius: 10px;
    border-right: 4px solid #007bff;
    margin: 20px 0;
}

.language-badge {
    background-color: #6c757d;
    color: white;
    padding: 4px 8px;
    border-radius: 15px;
    font-size: 12px;
    margin: 0 5px;
}

.arabic-lang { background-color: #dc3545; }
.darija-lang { background-color: #fd7e14; }
.french-lang { background-color: #ffc107; color: black; }
.english-lang { background-color: #28a745; }
</style>
""", unsafe_allow_html=True)

# API configuration
API_BASE_URL = "http://localhost:5000"

def reshape_arabic_text(text):
    """Reshape Arabic text for proper display"""
    try:
        reshaped_text = arabic_reshaper.reshape(text)
        return bidi.algorithm.get_display(reshaped_text)
    except:
        return text

def get_risk_color(risk_level):
    """Get color based on risk level"""
    colors = {
        'very_low': '#2ecc71',
        'low': '#27ae60', 
        'medium': '#f39c12',
        'high': '#e74c3c',
        'very_high': '#8e44ad'
    }
    return colors.get(risk_level, '#95a5a6')

def get_risk_emoji(risk_level):
    """Get emoji based on risk level"""
    emojis = {
        'very_low': 'âœ…',
        'low': 'ğŸŸ¢',
        'medium': 'ğŸŸ¡', 
        'high': 'ğŸŸ ',
        'very_high': 'ğŸ”´'
    }
    return emojis.get(risk_level, 'â“')

def create_risk_gauge(risk_score, risk_level):
    """Create a gauge chart for risk score"""
    fig = go.Figure(go.Indicator(
        mode = "gauge+number+delta",
        value = risk_score * 100,
        domain = {'x': [0, 1], 'y': [0, 1]},
        title = {'text': "Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø®Ø§Ø·Ø±Ø©"},
        delta = {'reference': 50},
        gauge = {
            'axis': {'range': [None, 100]},
            'bar': {'color': get_risk_color(risk_level)},
            'steps': [
                {'range': [0, 30], 'color': "lightgray"},
                {'range': [30, 60], 'color': "gray"},
                {'range': [60, 100], 'color': get_risk_color(risk_level)}
            ],
            'threshold': {
                'line': {'color': "red", 'width': 4},
                'thickness': 0.75,
                'value': 70
            }
        }
    ))
    
    fig.update_layout(
        height=300,
        font={'color': "darkblue", 'family': "Tajawal"}
    )
    
    return fig

def create_feature_importance_chart(feature_importance):
    """Create horizontal bar chart for feature importance"""
    if not feature_importance:
        return None
    
    # Prepare data
    features = [item['feature'] for item in feature_importance[:10]]
    importances = [item['importance'] for item in feature_importance[:10]]
    
    # Reshape Arabic text
    features_display = [reshape_arabic_text(f) for f in features]
    
    fig = go.Figure(go.Bar(
        x=importances,
        y=features_display,
        orientation='h',
        marker_color=[
            '#e74c3c' if imp > 0.7 else '#f39c12' if imp > 0.4 else '#2ecc71'
            for imp in importances
        ]
    ))
    
    fig.update_layout(
        title="Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª",
        xaxis_title="Ø§Ù„Ø£Ù‡Ù…ÙŠØ©",
        yaxis_title="Ø§Ù„Ù…ÙŠØ²Ø©",
        height=400,
        font={'family': 'Tajawal'},
        yaxis={'categoryorder': 'total ascending'}
    )
    
    return fig

def create_language_pie_chart(language_distribution):
    """Create pie chart for language distribution"""
    if not language_distribution:
        return None
    
    # Prepare data
    languages = list(language_distribution.keys())
    ratios = list(language_distribution.values())
    
    # Language labels in Arabic
    lang_labels = {
        'arabic': 'Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰',
        'darija': 'Ø§Ù„Ø¯Ø§Ø±Ø¬Ø©',
        'french': 'Ø§Ù„ÙØ±Ù†Ø³ÙŠØ©',
        'english': 'Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©',
        'unknown': 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ'
    }
    
    labels_display = [lang_labels.get(lang, lang) for lang in languages]
    
    fig = go.Figure(data=[go.Pie(
        labels=labels_display,
        values=ratios,
        hole=0.3
    )])
    
    fig.update_layout(
        title="ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù„ØºØ§Øª",
        font={'family': 'Tajawal'},
        height=300
    )
    
    return fig

def main():
    """Main Streamlit application"""
    
    # Header
    st.markdown("""
    <div class="rtl-text">
        <h1>ğŸ” Arabic/Darija Fake News Detection</h1>
        <p>Ù†Ø¸Ø§Ù… Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙƒØ§Ø°Ø¨Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¯Ø§Ø±Ø¬Ø©</p>
    </div>
    """, unsafe_allow_html=True)
    
    # Sidebar
    with st.sidebar:
        st.markdown("### âš™ï¸ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª")
        
        # Text input
        st.markdown("#### ğŸ“ Ø£Ø¯Ø®Ù„ Ø§Ù„Ù†Øµ Ù„Ù„ØªØ­Ù„ÙŠÙ„")
        input_text = st.text_area(
            "Ø§Ù„Ù†Øµ:",
            height=200,
            placeholder="Ø£Ø¯Ø®Ù„ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØ­Ù„ÙŠÙ„Ù‡ Ù‡Ù†Ø§...",
            key="input_text"
        )
        
        # Analysis options
        st.markdown("#### ğŸ”§ Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„")
        include_explanation = st.checkbox(
            "ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø´Ø±Ø­ Ø§Ù„Ù…ÙØµÙ„",
            value=True,
            help="Ù‚Ù… Ø¨ØªØ¶Ù…ÙŠÙ† Ø´Ø±Ø­ Ù…ÙØµÙ„ Ù„Ù„Ù‚Ø±Ø§Ø±"
        )
        
        use_fallback = st.checkbox(
            "Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ",
            value=True,
            help="Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ XGBoost ÙƒØ§Ø­ØªÙŠØ§Ø·ÙŠ Ø¹Ù†Ø¯ ÙØ´Ù„ AraBERT"
        )
        
        # Analyze button
        analyze_button = st.button(
            "ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ",
            type="primary",
            use_container_width=True
        )
        
        # Sample texts
        st.markdown("#### ğŸ“‹ Ù†ØµÙˆØµ Ø¹ÙŠÙ†Ø©")
        
        sample_texts = {
            "Ø®Ø¨Ø± Ø­Ù‚ÙŠÙ‚ÙŠ": "Ø£Ø¹Ù„Ù†Øª ÙˆØ²Ø§Ø±Ø© Ø§Ù„ØµØ­Ø© Ø§Ù„ÙŠÙˆÙ… Ø¹Ù† Ù†Ø¬Ø§Ø­ Ø­Ù…Ù„Ø© Ø§Ù„ØªØ·Ø¹ÙŠÙ… Ø¶Ø¯ ÙÙŠØ±ÙˆØ³ ÙƒÙˆØ±ÙˆÙ†Ø§ ÙÙŠ Ø¹Ø¯Ø© Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ø¨Ù„Ø§Ø¯.",
            "Ø®Ø¨Ø± ÙƒØ§Ø°Ø¨": "ØµØ¯Ù…Ø©: ÙƒØ´Ù Ø£Ø·Ø¨Ø§Ø¡ Ø³Ø± Ø¹Ù„Ø§Ø¬Ø§Ù‹ Ø³Ø­Ø±ÙŠØ§Ù‹ ÙŠÙ‚Ø¶ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø±Ø·Ø§Ù† ÙÙŠ Ø£Ø³Ø¨ÙˆØ¹ ÙˆØ§Ø­Ø¯ ÙÙ‚Ø·! Ù„Ø§ ØªØµØ¯Ù‚ÙˆØ§ Ù‡Ø°Ø§ Ø§Ù„Ø®Ø¨Ø± Ø§Ù„ÙƒØ§Ø°Ø¨.",
            "Ù†Øµ Ø¯Ø§Ø±Ø¬Ø©": "ÙƒØ§ÙŠÙ† ÙˆØ§Ø­Ø¯ ÙƒØ§ÙŠÙ‚ÙˆÙ„ Ù„ÙŠÙƒ Ù‡Ø§Ø¯ Ø§Ù„Ø®Ø¨Ø± ØµØ­ÙŠØ­ØŒ Ø£Ù†Ø§ Ù…Ø§ Ø¹Ø±ÙØªØ´ ÙˆØ§Ù„ÙˆØŒ Ø¯Ø§Ø¨Ø§Ø§ Ø§Ù„Ø²ÙŠÙ† Ù…Ø§Ø´ÙŠ Ù…Ø²ÙŠØ§Ù†.",
            "Ù†Øµ Ù…Ø®ØªÙ„Ø·": "Breaking news! Ø­Ø§Ø¯Ø« Ø®Ø·ÙŠØ± ÙÙŠ Ø§Ù„Ø¯Ø§Ø± Ø§Ù„Ø¨ÙŠØ¶Ø§Ø¡ØŒ according to sources multiples, il y aurait des victimes."
        }
        
        for sample_name, sample_text in sample_texts.items():
            if st.button(sample_name):
                st.session_state.input_text = sample_text
    
    # Main content area
    if analyze_button or input_text:
        if not input_text.strip():
            st.error("âš ï¸ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ù„Ù„ØªØ­Ù„ÙŠÙ„")
            return
        
        # Show loading
        with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„..."):
            try:
                # Call API
                response = requests.post(
                    f"{API_BASE_URL}/analyze",
                    json={
                        "text": input_text,
                        "include_explanation": include_explanation,
                        "use_fallback": use_fallback
                    },
                    timeout=30
                )
                
                if response.status_code == 200:
                    result = response.json()
                    
                    if result.get('success'):
                        analysis_result = result.get('result', {})
                        display_analysis_results(analysis_result, input_text)
                    else:
                        st.error(f"âŒ ÙØ´Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {result.get('error', 'Ø®Ø·Ø£ ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}")
                        
                else:
                    st.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø®Ø§Ø¯Ù…: {response.status_code}")
                    
            except requests.exceptions.Timeout:
                st.error("â° Ø§Ù†ØªÙ‡Øª Ù…Ù‡Ù„Ø© Ø§Ù„Ø§ØªØµØ§Ù„. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.")
            except Exception as e:
                st.error(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {str(e)}")
    
    # Footer info
    st.markdown("---")
    st.markdown("""
    <div class="rtl-text">
        <p><strong>ğŸ”¬ Ø¹Ù† Ø§Ù„Ù†Ø¸Ø§Ù…:</strong></p>
        <ul>
            <li>ÙŠØ¯Ø¹Ù… Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ØŒ Ø§Ù„Ø¯Ø§Ø±Ø¬Ø©ØŒ Ø§Ù„ÙØ±Ù†Ø³ÙŠØ©ØŒ ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©</li>
            <li>ÙŠØ³ØªØ®Ø¯Ù… Haqiqa API Ù…Ø¹ ØªØ­Ù„ÙŠÙ„ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ù…ÙŠØ²Ø§Øª</li>
            <li>ÙŠÙˆÙØ± Ø´Ø±Ø­ Ù…ÙØµÙ„ Ù„Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª LIME-like</li>
            <li>ÙˆØ§Ø¬Ù‡Ø© RTL Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø§Ù„ÙƒØ§Ù…Ù„</li>
        </ul>
        
        <p><strong>ğŸ“Š ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:</strong></p>
        <ol>
            <li>Ø£Ø¯Ø®Ù„ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø§Ø¯ ØªØ­Ù„ÙŠÙ„Ù‡</li>
            <li>Ø§Ø®ØªØ± Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©</li>
            <li>Ø§Ø¶ØºØ· Ø¹Ù„Ù‰ Ø²Ø± "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ"</li>
            <li>Ø§Ø³ØªØ¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…ÙØµÙ„Ø©</li>
        </ol>
        
        <p><strong>âš ï¸ Ù…Ù„Ø§Ø­Ø¸Ø§Øª:</strong></p>
        <ul>
            <li>Ø§Ù„Ù†Ø¸Ø§Ù… ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Haqiqa API Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ÙƒØ§Ø°Ø¨Ø©</li>
            <li>Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø¶Ø¹ Ø«ÙˆØ§Ù†Ù Ø­Ø³Ø¨ Ø·ÙˆÙ„ Ø§Ù„Ù†Øµ</li>
            <li>Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù‡ÙŠ Ù„Ø£ØºØ±Ø§Ø¶ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙÙ‚Ø· ÙˆÙ„Ø§ ØªØºÙ†ÙŠ Ø¹Ù† Ø§Ù„ÙŠÙ‚ÙŠÙ† Ø§Ù„Ù…Ø·Ù„Ù‚</li>
        </ul>
    </div>
    """, unsafe_allow_html=True)

def display_analysis_results(result, original_text):
    """Display comprehensive analysis results"""
    
    # Risk analysis section
    st.markdown("## ğŸ¯ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„")
    
    # Risk score and level
    risk_analysis = result.get('risk_analysis', {})
    risk_score = risk_analysis.get('overall_risk_score', 0)
    risk_level = risk_analysis.get('risk_level', 'unknown')
    
    # Create columns for metrics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        risk_color = get_risk_color(risk_level)
        st.markdown(f"""
        <div class="metric-card risk-{risk_level}">
            <h3>{get_risk_emoji(risk_level)} Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù…Ø®Ø§Ø·Ø±Ø©</h3>
            <h2>{reshape_arabic_text(risk_level.upper())}</h2>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown(f"""
        <div class="metric-card">
            <h3>ğŸ“Š Ø¯Ø±Ø¬Ø© Ø§Ù„Ù…Ø®Ø§Ø·Ø±Ø©</h3>
            <h2>{risk_score:.3f}</h2>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        haqiqa_score = risk_analysis.get('haqiqa_score', 0)
        st.markdown(f"""
        <div class="metric-card">
            <h3>ğŸ¤– Haqiqa</h3>
            <h2>{haqiqa_score:.3f}</h2>
        </div>
        """, unsafe_allow_html=True)
    
    with col4:
        feature_score = risk_analysis.get('feature_score', 0)
        st.markdown(f"""
        <div class="metric-card">
            <h3>ğŸ“ˆ Ø§Ù„Ù…ÙŠØ²Ø§Øª</h3>
            <h2>{feature_score:.3f}</h2>
        </div>
        """, unsafe_allow_html=True)
    
    # Risk gauge
    st.plotly_chart(create_risk_gauge(risk_score, risk_level), use_container_width=True)
    
    # Language analysis
    language_analysis = result.get('language_analysis', {})
    if language_analysis:
        st.markdown("### ğŸŒ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù„ØºØ©")
        
        lang_col1, lang_col2 = st.columns(2)
        
        with lang_col1:
            primary_lang = language_analysis.get('primary_language', 'unknown')
            confidence = language_analysis.get('confidence', 0)
            
            # Language badge
            lang_class = f"{primary_lang}-lang"
            st.markdown(f"""
            <span class="language-badge {lang_class}">
                {reshape_arabic_text(primary_lang.upper())}
            </span>
            <span style="margin-right: 10px;">
                Ø§Ù„Ø«Ù‚Ø©: {confidence:.1%}
            </span>
            """, unsafe_allow_html=True)
        
        with lang_col2:
            is_code_switched = language_analysis.get('is_code_switched', False)
            st.markdown(f"""
            <div class="explanation-box">
                <h4>ğŸ”„ Code-switching:</h4>
                <p>{'Ù†Ø¹Ù…' if is_code_switched else 'Ù„Ø§'}</p>
            </div>
            """, unsafe_allow_html=True)
        
        # Language distribution chart
        lang_dist = language_analysis.get('language_distribution', {})
        if lang_dist:
            st.plotly_chart(create_language_pie_chart(lang_dist), use_container_width=True)
    
    # Feature analysis
    feature_analysis = result.get('feature_analysis', {})
    if feature_analysis:
        st.markdown("### ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ÙŠØ²Ø§Øª")
        
        # Tabs for different feature types
        feature_tab1, feature_tab2, feature_tab3 = st.tabs(["ğŸ“ Ø§Ù„Ù†Øµ", "ğŸ˜€ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±", "ğŸ” Ø§Ù„Ù…Ø¹Ø¬Ù…"])
        
        with feature_tab1:
            text_features = feature_analysis.get('text_features', {})
            if text_features:
                st.json(text_features)
        
        with feature_tab2:
            sentiment_features = feature_analysis.get('sentiment_features', {})
            if sentiment_features:
                # Sentiment metrics
                sent_col1, sent_col2, sent_col3 = st.columns(3)
                
                with sent_col1:
                    positive_score = sentiment_features.get('positive_score', 0)
                    st.metric("Ø¥ÙŠØ¬Ø§Ø¨ÙŠ", f"{positive_score:.3f}")
                
                with sent_col2:
                    negative_score = sentiment_features.get('negative_score', 0)
                    st.metric("Ø³Ù„Ø¨ÙŠ", f"{negative_score:.3f}")
                
                with sent_col3:
                    subjectivity = sentiment_features.get('sentiment_subjectivity', 0)
                    st.metric("Ù…ÙˆØ¶ÙˆØ¹ÙŠØ©", f"{subjectivity:.3f}")
                
                st.json(sentiment_features)
        
        with feature_tab3:
            lexicon_features = feature_analysis.get('lexicon_features', {})
            if lexicon_features:
                # Lexicon risk factors
                risk_factors = lexicon_features.get('overall_fake_news_risk', 0)
                st.metric("Ù…Ø®Ø§Ø·Ø± Ø§Ù„Ù…Ø¹Ø¬Ù…", f"{risk_factors:.3f}")
                
                st.json(lexicon_features)
    
    # Haqiqa prediction
    haqiqa_prediction = result.get('haqiqa_prediction', {})
    if haqiqa_prediction:
        st.markdown("### ğŸ¤– ØªÙ†Ø¨Ø¤ Haqiqa")
        
        haqiqa_col1, haqiqa_col2 = st.columns(2)
        
        with haqiqa_col1:
            prediction = haqiqa_prediction.get('prediction', 'Unknown')
            confidence = haqiqa_prediction.get('confidence', 0)
            
            st.markdown(f"""
            <div class="explanation-box">
                <h4>ğŸ¯ Ø§Ù„ØªÙ†Ø¨Ø¤:</h4>
                <h3>{reshape_arabic_text(prediction)}</h3>
                <p>Ø§Ù„Ø«Ù‚Ø©: {confidence:.1%}</p>
                <p>Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {haqiqa_prediction.get('model_used', 'Unknown')}</p>
            </div>
            """, unsafe_allow_html=True)
        
        with haqiqa_col2:
            if haqiqa_prediction.get('fallback_used'):
                st.warning("âš ï¸ ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ (XGBoost)")
            
            # Probabilities
            real_prob = haqiqa_prediction.get('real_probability', 0)
            fake_prob = haqiqa_prediction.get('fake_probability', 0)
            
            st.markdown(f"""
            <div class="explanation-box">
                <h4>ğŸ“Š Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª:</h4>
                <p>Ø­Ù‚ÙŠÙ‚ÙŠ: {real_prob:.1%}</p>
                <p>ÙƒØ§Ø°Ø¨: {fake_prob:.1%}</p>
            </div>
            """, unsafe_allow_html=True)
    
    # Explanation
    explanation = result.get('explanation')
    if explanation and st.session_state.get('include_explanation', True):
        st.markdown("### ğŸ“ Ø§Ù„Ø´Ø±Ø­ Ø§Ù„Ù…ÙØµÙ„")
        
        exp_col1, exp_col2 = st.columns(2)
        
        with exp_col1:
            st.markdown(f"""
            <div class="explanation-box">
                <h4>ğŸ“‹ Ù…Ù„Ø®Øµ Ø§Ù„Ø´Ø±Ø­:</h4>
                <p>{reshape_arabic_text(explanation.get('summary', ''))}</p>
            </div>
            """, unsafe_allow_html=True)
        
        with exp_col2:
            key_factors = explanation.get('key_factors', [])
            if key_factors:
                st.markdown("**Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ù…Ø®Ø§Ø·Ø±Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©:**")
                for i, factor in enumerate(key_factors[:5]):
                    factor_name = factor.get('factor', 'unknown')
                    severity = factor.get('severity', 'unknown')
                    impact = factor.get('impact', 0)
                    
                    st.markdown(f"""
                    <div style="margin: 10px 0; padding: 10px; background-color: #f8f9fa; border-radius: 5px;">
                        <strong>{i+1}. {reshape_arabic_text(factor_name)}</strong><br>
                        <small>Ø§Ù„Ø´Ø¯Ø©: {severity} | Ø§Ù„ØªØ£Ø«ÙŠØ±: {impact:.3f}</small>
                    </div>
                    """, unsafe_allow_html=True)
        
        # Recommendations
        recommendations = explanation.get('recommendations', [])
        if recommendations:
            st.markdown("**ğŸ“Œ Ø§Ù„ØªÙˆØµÙŠØ§Øª:**")
            for rec in recommendations:
                st.markdown(f"- {reshape_arabic_text(rec)}")
    
    # Processing time
    processing_time = result.get('processing_time', 0)
    if processing_time:
        st.markdown(f"â±ï¸ ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {processing_time:.2f} Ø«Ø§Ù†ÙŠØ©")
    
    # Feature importance chart
    if feature_analysis:
        lexicon_features = feature_analysis.get('lexicon_features', {})
        if lexicon_features:
            # Create feature importance from lexicon
            feature_importance = [
                {'feature': 'clickbait', 'importance': lexicon_features.get('clickbait_score', 0)},
                {'feature': 'Ø¹Ø¯Ù… Ø§Ù„ÙŠÙ‚ÙŠÙ†', 'importance': lexicon_features.get('uncertainty_score', 0)},
                {'feature': 'Ù†Ø¸Ø±ÙŠØ§Øª Ø§Ù„Ù…Ø¤Ø§Ù…Ø±Ø©', 'importance': lexicon_features.get('conspiracy_score', 0)},
                {'feature': 'Ø§Ù„Ø¯Ø¹Ø§ÙŠØ©', 'importance': lexicon_features.get('propaganda_score', 0)},
                {'feature': 'Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø¶Ù„Ù„Ø©', 'importance': lexicon_features.get('unreliable_source_score', 0)}
            ]
            
            feature_chart = create_feature_importance_chart(feature_importance)
            if feature_chart:
                st.plotly_chart(feature_chart, use_container_width=True)

if __name__ == "__main__":
    main()